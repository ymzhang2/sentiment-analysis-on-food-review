
# coding: utf-8

# In[1]:

get_ipython().magic(u'matplotlib inline')

import sqlite3
import pandas as pd
import numpy as np
import nltk
import string
import matplotlib.pyplot as plt
import numpy as np



df = sqlite3.connect('/Users/yimanzhang/Desktop/dataset/database.sqlite')
messages = pd.read_sql_query("""
SELECT result, comment
FROM Reviews
WHERE result != 3
""", df)

def split(x):
    if x < 3:
        return 'negative'
    return 'positive'

result = messages['result']
result = result.map(split)
comment = messages['comment']
X_train, X_test, y_train, y_test = train_test_split(comment, result, test_size=0.3, random_state=1)

stemmer = PorterStemmer()

import re

def replace_non_alphanumerics(source):
    result = re.sub("[^_a-zA-Z0-9]", ' ', source)
    return result
    
def token_stem(tokens, stemmer):
    stemmed = []
    for item in tokens:
        stemmed.append(stemmer.stem(item))
    return stemmed

def tokenize(text):
    tokens = nltk.word_tokenize(text)
    #tokens = [word for word in tokens if word not in stopwords.words('english')]
    stems = token_stem (tokens, stemmer)
    return ' '.join(stems)


## countVectorizer: Text preprocessing, tokenizing and filtering of stopwords are included in a high level component 
## that is able to build a dictionary of features and transform documents to feature vectors:
## tfidf_transformer.fit_transform: Occurrence count is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.
## To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called tf for Term Frequencies.
## Another refinement on top of tf is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.
## we firstly use the fit(..) method to fit our estimator to the data and secondly the transform(..) method to transform our count-matrix to a tf-idf representation. 
## These two steps can be combined to achieve the same end result faster by skipping redundant processing.
## This is done through using the fit_transform(..) method as shown below, and as mentioned in the note in the previous section:

##--- Training set

corpus = []
for w in X_train:
    w = w.lower()
    w = replace_non_alphanumerics(w)
    w=tokenize(w)
    corpus.append(w)
        
count_vect = CountVectorizer()
X_train_counts = count_vect.fit_transform(corpus)        
        
tfidf_transformer = TfidfTransformer()
X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)

#--- Test set

test_set = []
for w in X_test:
    w = text.lower()
    w = replace_non_alphanumerics(w)
    w=tokenize(w)
    w_set.append(w)

X_new_counts = count_vect.transform(test_set)
X_test_tfidf = tfidf_transformer.transform(X_new_counts)




